---
author: "JeffTheDon-Jeffery B. Donaven"
date: "April 5, 2018"
output:
  word_document: default
  keep_md: yes
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
setwd("C:/Users/geo/Desktop/Jeff/Coursera/Machine_Learning/week4")
library(ggplot2); library(caret); library(knitr); library(kableExtra)
```
<img src="machine learning.png" style="position:absolute;top:0px;right:0px;" />


# **Prediction of Errors in the Execution of Assorted Weight-Training Exercises**

## **Synopsis**
[Weight-Training](https://en.wikipedia.org/wiki/Weight_training) is common type of strength training utilizing the force of gravity in the form of weighted bars, dumbells or weight stacks so as to oppose the force generated by muscle through concentric or eccentric contraction.  The [study](http://groupware.les.inf.puc-rio.br/har) that this project is based on uses *accelerometers* that measure and record data so as to quantify how much of a particular activity that the test subjects had done. The **main objective** of this *project* is to determine whether or not it is possible to *classify errors* during the execution of the different exercises using the data gathered by the *accelerometers* and to predict the manner in which the subjects did each exercise.  I am using regressionary techniques as the tool in which will create predictive models on the [HAR-Dataset](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).  I have classified errors in and correct execution of, "**lifting barbells**" with *sensitivity*, *specificity*, and **HIGH** *accuracy*.

## **Introduction**

This study was performed on six male participants with ages between 20 and 28 years with little or minimal experience in regars to Weight-Training.  Each were asked to peform one set of Unilateral Dumbell Biceps Curl consisting of 10 repetitions in the set using a relatively light, 1.25kg dumbbell in different ways:

* **Class A** - *Exactly according to the specification of the exercise*
* **Class B** - *Throwing the elbows to the front with each repetition*
* **Class C** - *Lifting the Dumbbell only halfway up with each repetition*
* **Class D** - *Lowering the Dumbbell only halway down with each repetition*
* **Class E** - *Throwing the hips to the front with each repetition*

Mounted sensors on each of the participants were located in their respective **gloves**, on their **armbands**, in their **lumbar support belts** and in each of the **dumbbells** used.  All of which collected data on the "*Euler*" angles (**pitch, roll and yaw**).  Additionally, there were readings and measurements recorded via the **raw accelerator, gyroscope, and magnetometer**.  All of this information can be reviewed at the aforementioned [website](http://groupware.les.inf.puc-rio.br/har).

## **Downloading of Data**

As referenced earlier, the data for this course project comes from the [Human Activity Recognition DataSet](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) which is hosted by [Groupware@LES](http://groupware.les.inf.puc-rio.br/har):

```{r, data_download, cache = TRUE}
train_set_site <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_set_site <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(train_set_site, destfile = "training_set.csv")
download.file(test_set_site, destfile = "testing_set.csv")
date_of_download <- date()
date_of_download
```

## **Reading of and Preprocessing the Dataset:**
```{r, read_data_process, results = "asis", cache = TRUE}
train_set <- read.csv("training_set.csv", header = TRUE, na.strings = c("NA", "#DIV/0!", ""), stringsAsFactors = FALSE)
test_set <- read.csv("testing_set.csv", header = TRUE, na.strings = c("NA", "#DIV/0!", ""), stringsAsFactors = FALSE)
```

Upon reviewing this data, the features of which *can* be classified into 3 specific Variables: **Measurement**, **Summary** and **"HouseKeeping"**. When looking at the "*summary*" variables, I can see they begin with, "**amplitude, avg, kurtosis, min, max, skewness, and  stddev**".  These apply summary statistics on the "*measurement*" variables which begin with, "**accel, gyros, magnet, roll, pitch and yaw**".  The "*summary*" variables would obviously be the choice variable for my model because they would have drastically cut down on the number of observations and processing time and they also contain the core *measurement* variables.  Unfortunately, the *"test_set"* contains only missing values, so making predictions using the *summary* variable would be NULL.  Oh well.

I will be removing the **"HouseKeeping"** variables that contain the row numbers **"X"**, the time_stamps(**""raw_timestamp_part_1", "raw_timestamp_part_2"", "cvtd_timestamp"**), and the measurement intervals, "**new_window and num_window**".

```{r, prep_processing_data, cache = TRUE}
sum_variables_indexed <- grepl("^amplitude|^avg|^kurtosis|^min|^max|^skewness|^stddev"
                               , names(train_set))
sum_variables <- names(train_set)[!sum_variables_indexed]
train_df <- train_set[, sum_variables]
na_be_gone_index <- sapply(train_df, function(x)sum(is.na(x)))
fin_train_df <- train_df[, -c(1:7)]
fin_train_df <- fin_train_df[, colSums(is.na(fin_train_df))==0]
```

Furthermore, the **train_set** contains ```r dim(train_set)[1]``` *rows* and ```r dim(train_set)[2]``` *variables*.  Conversely, the **test_set** contains ```r dim(test_set)[1]``` *rows* and ```r dim(test_set)[2]``` variables. I also need to set the variables to their respective correct class so as to avoid making errors during the modeling phase.

### *Setting of Variables*
```{r, set_variable_class}
fin_train_df$accel_arm_x <- as.numeric(fin_train_df$accel_arm_x)
fin_train_df$accel_arm_y <- as.numeric(fin_train_df$accel_arm_y)
fin_train_df$accel_arm_z <- as.numeric(fin_train_df$accel_arm_z)
fin_train_df$total_accel_arm <- as.numeric(fin_train_df$total_accel_arm)
fin_train_df$accel_belt_x <- as.numeric(fin_train_df$accel_belt_x)
fin_train_df$accel_belt_y <- as.numeric(fin_train_df$accel_belt_y)
fin_train_df$accel_belt_z <- as.numeric(fin_train_df$accel_belt_z)
fin_train_df$total_accel_belt <- as.numeric(fin_train_df$total_accel_belt)
fin_train_df$accel_dumbbell_x <- as.numeric(fin_train_df$accel_dumbbell_x)
fin_train_df$accel_dumbbell_y <- as.numeric(fin_train_df$accel_dumbbell_y)
fin_train_df$accel_dumbbell_z <- as.numeric(fin_train_df$accel_dumbbell_z)
fin_train_df$total_accel_dumbbell <- as.numeric(fin_train_df$total_accel_dumbbell)
fin_train_df$accel_forearm_x <- as.numeric(fin_train_df$accel_forearm_x)
fin_train_df$accel_forearm_y <- as.numeric(fin_train_df$accel_forearm_y)
fin_train_df$accel_forearm_z <- as.numeric(fin_train_df$accel_forearm_z)
fin_train_df$total_accel_forearm <- as.numeric(fin_train_df$total_accel_forearm)
fin_train_df$magnet_arm_x <- as.numeric(fin_train_df$magnet_arm_x)
fin_train_df$magnet_arm_y <- as.numeric(fin_train_df$magnet_arm_y)
fin_train_df$magnet_arm_z <- as.numeric(fin_train_df$magnet_arm_z)
fin_train_df$magnet_belt_x <- as.numeric(fin_train_df$magnet_belt_x)
fin_train_df$magnet_belt_y <- as.numeric(fin_train_df$magnet_belt_y)
fin_train_df$magnet_belt_z <- as.numeric(fin_train_df$magnet_belt_z)
fin_train_df$magnet_dumbbell_x <- as.numeric(fin_train_df$magnet_dumbbell_x)
fin_train_df$magnet_dumbbell_y <- as.numeric(fin_train_df$magnet_dumbbell_y)
fin_train_df$magnet_forearm_x <- as.numeric(fin_train_df$magnet_forearm_x)

fin_train_df$classe <- as.factor(fin_train_df$classe)
```

### *Checking Variables which contain zeroes*
```{r, zeroed_out}
zed_index <- sapply(fin_train_df[,-53], sum)
zed_variables <- which(zed_index == 0)
zed_variables
fin_train_df <- fin_train_df[-c(845, 867, 899, 7058, 8468, 9029, 9264, 11989, 17508),]
```

## **Creation of Training, Test & Validation sets of Data**

So as to truly test this data in my models-to-come, I am going to partition the dataset into 3 groups; one training set at 50%, one test set at 30%, and one validation set at 20%.  The downloaded *test_set* from earlier will be the **final** validation of my models.
```{r, partition_of_data}
set.seed(2018)
part_1 <- createDataPartition(y = fin_train_df$classe, p = .50, list = FALSE)
train_1_set <- fin_train_df[part_1,]
validation <- fin_train_df[-part_1,]
part_2 <- createDataPartition(y = validation$classe, p = 0.70, list = FALSE)
test_1_set <- validation[part_2,]
validate_1_set <- validation[-part_2,]
```

Below you will see a table showing the final datasets and how they are comprised:
```{r, data_table_of_datasets_for_models, echo = FALSE}
table_1 <- data.frame("Type_of_DataSet" = c("Training DataSet","Testing DataSet", "Validation DataSet")
                      , "Number of Variables" = c(dim(train_1_set)[2], dim(test_1_set)[2], dim(validate_1_set)[2])
                      , "Number of Rows" = c(dim(train_1_set)[1], dim(test_1_set)[1], dim(validate_1_set)[1]))
x <- kable(table_1, caption = "Breakdown of Training/Testing & Validation DataSets", align = "c")
kable_styling(x, "striped", position = "left", font_size = 10)
```
        
## **Creation of Models**

I will begin by generating a Random Forest Model on the Training Dataset.  Per the Instructions for this Project, I will be using the variable, "classe" as my dependent variable.  The "classe" variable contains the important classification on whether or not the movement through the exercise was performed correctly.  Additionally, it also provides me with what error was committed as per the "Introduction" section earlier.  I am also including a 5-fold cross validation to improve my model as well as repeating it three times.
```{r, model_1, cache = TRUE, results = "hide", warning = FALSE, message = FALSE}
train_model_control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
fit_all_train_model = train(classe ~ ., data = train_1_set, method = "rf", trControl = train_model_control)
```

### Assessment of Model 1 - Training DataSet

Upon examination of the results, I find it to be pretty accurate with a value of approximately 98.5%.
```{r, model_1_results_1a}, results = "asis", echo = FALSE}
model_1_results <- kable(fit_all_train_model$results)
kable_styling(model_1_results, "striped", position = "left", font_size = 10)
```


The table below shows which predictions of the *Training DataSet* were correct, and which were errors via the  **error_in_sample**.  The errors are represented by the numbers which are not on the diagonal from top left to bottom right.  The sum of errors on the Training DataSet numbered **96** out of a total **9790** which has a misclassifaction rate of **0.98%**.  The Accuracy of correct classifications in my model is **99.02%**.
```{r model_1_results_1b, echo = FALSE}
err_in_train_sample <- fit_all_train_model$finalModel$confusion
table_2 <- kable(err_in_train_sample)
kable_styling(table_2, "striped", position = "left", font_size = 10)
```

I expect the error out of sample to be slightly less than the error in sample that was measured above.
Now lets see how the predictions go against the Testing DataSet:
```{r, pred_model_2, results = "hide", warning = FALSE, message = FALSE}
pred_test_set <- predict(fit_all_train_model, newdata = test_1_set)
err_out_of_sample <- table(pred_test_set, test_1_set$classe)
accuracy_of_model <- confusionMatrix(pred_test_set, test_1_set$classe)
```
```{r, model_2_results, results = "asis", echo = FALSE}
table_3 <- kable(err_out_of_sample)
kable_styling(table_3, "striped", position = "left", font_size = 10)
```

As predicted, the **error out of sample** for the *Test DataSet* via the table produced above, shows a misclassification rate of **1.22%** with an Accuracy Rate for this DataSet of **98.78%** with **84** misclassifacations out of a total possible **6866**.  This confirmed my hypothesis of it being less accurate than the **error in sample**.

The **confusionMatrix** function summarizes in detail the **accuracy, sensitivity, specificty** as well as other *parameters* of my model's prediction by *class*.  Let's take a look at this:
```{r, accuracy_of_model_table, echo = FALSE}
table_4 <- kable(accuracy_of_model$byClass)
kable_styling(table_4, "striped", position = "left", font_size = 10)
```


I have created a plot to showcase the relationship that exists between the *number of randomly selected predictors* and the *accuracy*.  Accuracy of the model is at its highest point, when, **mtry**(the tuning parameter for caret) has the number of variables available for splitting at each tree node is at 27.
["mtry"](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) is defined as, 'The number of Variables randomly sampled as canditates for each split.'  Keep this in mind whilst viewing the plot *below*:
```{r, accuracy_plot}
plot(fit_all_train_model)
```

Next I want to check which features are highly **correlated** so as to decide which features to keep for the next model run.  I want to hone the model so as to improve *processing time*, *scalability* as well as *interpretability*.  Therefore I am going to take a look at the Features and see just how important each one is:
```{r, feature_plot, fig.height = 6, fig.width = 5, echo = FALSE}
importance <- varImp(fit_all_train_model, scale = FALSE)
plot(importance)
```
```{r, correlated_variables, echo = FALSE}
cor_vars <- cor(train_1_set[,-53])
summary_of_cor_vars <- summary(cor_vars[upper.tri(cor_vars)])
higher_cor<- findCorrelation(cor_vars, cutoff= .80)
higher_cor_vars <- as.data.frame(names(train_1_set)[higher_cor])
cor_table <- kable(higher_cor_vars)
kable_styling(cor_table, "striped", position = "left", font_size = 10)
```
So, can I gain anymore precision in my model with the above found correlated variables?  I shall run another model to see if it is possible with the above listed 13 variables which is the top 20% of variables to find out:

```{r, final_model_13, cache = TRUE, results = "hide"}
final_model <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
fit_train_13_model= train(classe ~ accel_belt_z + roll_belt + accel_belt_y + accel_dumbbell_x + accel_belt_x + pitch_belt + accel_dumbbell_x + accel_arm_x + magnet_arm_y + gyros_forearm_y + gyros_dumbbell_x + gyros_dumbbell_z + gyros_arm_x, data = train_1_set, method = "rf", trControl = final_model)
```
```{r, results_model_13, results = "asis", warning = FALSE, message = FALSE}

pred_model_13 <- predict(fit_train_13_model, newdata = test_1_set)
accuracy_model_13 <- confusionMatrix(pred_model_13, test_1_set$classe)
```

```{r, table_5_model_13, echo = FALSE}
table_5 <- kable(accuracy_model_13$byClass)
kable_styling(table_5, "striped", position = "left", font_size = 10)
```

Well, running the model with the 13 correlated variables showed a slight decrease in accuracy, specificity and sensitivity.  Although the numbers went down, I want to run one more model, this time with only 6 variables to see if I can get a rise with the numbers.  I think the numbers will continue to go down a bit though.

```{r, final_model_six, cache = TRUE, results = "hide", warning = FALSE, message = FALSE}
fit_train_6_model = train(classe ~ accel_belt_z + roll_belt + accel_belt_y + accel_dumbbell_x + accel_belt_x + pitch_belt, data = train_1_set, method = "rf", trControl = final_model)
```
```{r, results_model_6, results = "asis"}
pred_model_6 <- predict(fit_train_6_model, newdata = test_1_set)
accuracy_model_6 <- confusionMatrix(pred_model_6, test_1_set$classe)
```
```{r, table_6_model_6, echo = FALSE}
table_6 <- kable(accuracy_model_6$byClass)
kable_styling(table_6, "striped", position = "left", font_size = 10)
```

As predicted, sensitivity, specificity and accuracy all suffered from a decrease in the numbers with 6 variables.  However, this model has better interpreability, scalability and faster processing time even though there is an increase in bias which reduced my capacity to predict accurately.

## Validation Set Model Fitting

Now I shall run the model on the validation test set to see how accurate it is.  I will be running the model with all the variables and the one with only 6 to see the disparity between the two.

```{r, validation_tests, cache = TRUE, results = "asis", warning = FALSE, message = FALSE}
pred_val_test_all <- predict(fit_all_train_model, newdata = validate_1_set)
accuracy_val_all <-confusionMatrix(pred_val_test_all, validate_1_set$classe)
pred_model_6b <- predict(fit_train_6_model, newdata = validate_1_set)
accuracy_model_6b <- confusionMatrix(pred_model_6b, validate_1_set$classe)
```
```{r, table_val_all_vs_model_6b, echo = FALSE}
table_val_all <- kable(accuracy_val_all$byClass)
kable_styling(table_val_all, "striped", position = "left", font_size = 10)
table_model_6b <- kable(accuracy_model_6b$byClass)
kable_styling(table_model_6b, "striped", position = "left", font_size = 10)
```

## Conclusions

Classification of Errors using predictive modeling on the **HAR Weightlifting Dataset** showed high **sensitivity**, **specificity** and **accuracy** with the correct execution of lifting the dumbbells.  It needds to be pointed out though, that the **errors** in the movement of the excercises were performed *purposefully*.  Because of this fact, different results could be retained when the errors in the movements are committed *without* intent to commit the error in the first place.

Please see the appendix for different plots which will show the misclassification of errors by the the models on the Validation Test Set.

## Predictions on the Test DataSet
I will now use the models created herein to Predict on the Downloaded Test DataSet:

```{r, test_dataset_predictions}
test_data <- test_set[ , which(names(test_set) %in% names(train_1_set))]
pred_test_all <- predict(fit_all_train_model, newdata = test_set)
print(pred_test_all)
```
```{r, test_dataset_13}
pred_test_13 <- predict(fit_train_13_model, newdata = test_set)
print(pred_test_13)
```
```{r, test_dataset_6}
pred_test_6 <- predict(fit_train_6_model, newdata = test_set)
print(pred_test_6)
```

## Appendix

```{r, plot_all_val_set}
val_all<- pred_val_test_all == validate_1_set$classe
qplot(pitch_belt, accel_belt_x, color = val_all, data = validate_1_set)
```

```{r, plot_13_val_set}
pred_val_13 <- predict(fit_train_13_model, newdata = validate_1_set)
accuracy_val_13 <- confusionMatrix(pred_val_13, validate_1_set$classe)
val_model_13 <- pred_val_13 == validate_1_set$classe
qplot(pitch_belt, accel_belt_x, color = val_model_13, data = validate_1_set)
```

```{r, plot_4_val_set}
pred_val_6 <- predict(fit_train_6_model, newdata = validate_1_set)
accuracy_val_6 <- confusionMatrix(pred_val_6, validate_1_set$classe)
val_model_6 <- pred_val_6 == validate_1_set$classe
qplot(pitch_belt, accel_belt_x, color = val_model_6, data = validate_1_set)
```